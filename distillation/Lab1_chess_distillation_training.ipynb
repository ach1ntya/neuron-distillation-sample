{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Chess Move Evaluation - Knowledge Distillation Training\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will train a smaller \"student\" model to evaluate chess moves using knowledge distillation. This builds on Lab 0, where you generated teacher model logits from the Qwen3-30B-A3B model.\n",
    "\n",
    "**Task**: Train a student model to classify which chess move is better (MoveA or MoveB)\n",
    "\n",
    "**Why Knowledge Distillation for Chess?**\n",
    "- **Cost Reduction**: 50x smaller model (30B → 0.6B parameters)\n",
    "- **Faster Inference**: ~20-50x faster move evaluation\n",
    "- **Deployment Flexibility**: Can run on smaller instances or edge devices\n",
    "- **Maintained Performance**: Retains much of the teacher's chess understanding\n",
    "\n",
    "**Training Approach:**\n",
    "\n",
    "The `KnowledgeDistillationTrainer` combines two loss functions:\n",
    "1. **Hard Loss**: Cross-entropy with true labels (MoveA or MoveB)\n",
    "2. **Soft Loss**: KL divergence between teacher and student logits\n",
    "\n",
    "Combined loss: `total_loss = α × soft_loss + (1 - α) × hard_loss`\n",
    "\n",
    "Where α=0.7 means 70% weight on learning from teacher, 30% on correct answers.\n",
    "\n",
    "**Models:**\n",
    "- **Teacher**: Qwen3-30B-A3B (30 billion parameters)\n",
    "- **Student**: Qwen3-0.6B (600 million parameters)\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed Lab 0 with chess logits saved to `data/chess_output.json`\n",
    "- AWS Trainium instance (trn1.32xlarge recommended)\n",
    "- AWS Neuron SDK installed\n",
    "- Virtual environment: `/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Student Model\n",
    "\n",
    "Download the Qwen3-0.6B model weights from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q neuronx-distributed datasets optimum-neuron[training]==0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf download Qwen/Qwen3-0.6B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Configure environment variables for optimal Neuron performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Neuron compiler and runtime settings\n",
    "os.environ['NEURON_CC_FLAGS'] = \"--model-type transformer --retry_failed_compilation\"\n",
    "os.environ['NEURON_FUSE_SOFTMAX'] = \"1\"\n",
    "os.environ['NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS'] = \"3\"\n",
    "os.environ['MALLOC_ARENA_MAX'] = \"64\"\n",
    "os.environ['WORLD_SIZE'] = \"8\"\n",
    "os.environ['WANDB_DISABLED'] = \"true\"  # Disable wandb logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Define hyperparameters for the distillation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "PROCESSES_PER_NODE = 4  # Distributed training processes\n",
    "NUM_EPOCHS = 3  # Number of training epochs\n",
    "TP_DEGREE = 2  # Tensor parallelism degree\n",
    "BS = 1  # Batch size per device\n",
    "GRADIENT_ACCUMULATION_STEPS = 16  # Effective batch size = 16\n",
    "LOGGING_STEPS = 1  # Log every step\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "OUTPUT_DIR = \"Qwen3-0.6B-chess-finetuned\"\n",
    "DATASET_PATH = \"data/chess_output.json\"\n",
    "\n",
    "# Distillation hyperparameters\n",
    "TEMPERATURE = 4.0  # Softness of probability distributions\n",
    "ALPHA = 0.7  # Weight for soft loss (0.7 = 70% teacher, 30% labels)\n",
    "\n",
    "# Set max steps (use -1 for full training)\n",
    "MAX_STEPS = -1  # Train for full epochs\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {DATASET_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Temperature: {TEMPERATURE}, Alpha: {ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Chess Dataset\n",
    "\n",
    "Check that the chess logits data from Lab 0 is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(DATASET_PATH).exists():\n",
    "    print(f\"ERROR: {DATASET_PATH} not found!\")\n",
    "    print(\"Please run Lab0_generate_teacher_logits_chess.ipynb first.\")\n",
    "else:\n",
    "    with open(DATASET_PATH, 'r') as f:\n",
    "        chess_data = json.load(f)\n",
    "    \n",
    "    valid_samples = [s for s in chess_data if 'error' not in s]\n",
    "    print(f\"✓ Found {len(valid_samples)} valid chess samples\")\n",
    "    print(f\"✓ Average logit positions: {sum(len(s['response']['token_logits']) for s in valid_samples) / len(valid_samples):.1f}\")\n",
    "    \n",
    "    # Show example\n",
    "    sample = valid_samples[0]\n",
    "    print(f\"\\nExample:\")\n",
    "    print(f\"  Input: {sample['input'][:100]}...\")\n",
    "    print(f\"  Expected: {sample['expected_output']}\")\n",
    "    print(f\"  Generated: {sample['response']['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "Execute the distributed training using `torchrun`.\n",
    "\n",
    "**Note**: First run will compile the model (~20-30 minutes). Subsequent runs use cached compilation.\n",
    "\n",
    "**Training Process:**\n",
    "1. **Compilation** (first run only): Neuron compiler optimizes model for Trainium\n",
    "2. **Training**: Student learns from teacher logits\n",
    "3. **Checkpointing**: Model saved to OUTPUT_DIR\n",
    "\n",
    "**Expected Time:**\n",
    "- Compilation: ~20-30 minutes (one-time)\n",
    "- Training (100 samples, 3 epochs): ~10-15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the training command\n",
    "training_cmd = f\"\"\"\n",
    "torchrun  \\\\\n",
    "    --nproc_per_node {PROCESSES_PER_NODE} \\\\\n",
    "    src/distill_chess_neuron_torchrun.py \\\\\n",
    "    --model_id {MODEL_NAME} \\\\\n",
    "    --dataset_path {DATASET_PATH} \\\\\n",
    "    --output_model_path ./final_chess_model \\\\\n",
    "    --temperature {TEMPERATURE} \\\\\n",
    "    --alpha {ALPHA} \\\\\n",
    "    --num_train_epochs {NUM_EPOCHS} \\\\\n",
    "    --do_train \\\\\n",
    "    --max_steps {MAX_STEPS} \\\\\n",
    "    --per_device_train_batch_size {BS} \\\\\n",
    "    --gradient_accumulation_steps {GRADIENT_ACCUMULATION_STEPS} \\\\\n",
    "    --learning_rate 1e-4 \\\\\n",
    "    --bf16 \\\\\n",
    "    --zero_1 False \\\\\n",
    "    --tensor_parallel_size {TP_DEGREE} \\\\\n",
    "    --warmup_steps 5 \\\\\n",
    "    --pipeline_parallel_size 1 \\\\\n",
    "    --logging_steps {LOGGING_STEPS} \\\\\n",
    "    --output_dir {OUTPUT_DIR} \\\\\n",
    "    --overwrite_output_dir\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"This will take ~30-45 minutes on first run (includes compilation)\")\n",
    "print(\"\\nCommand:\")\n",
    "print(training_cmd)\n",
    "\n",
    "# Run training\n",
    "!{training_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate the shards\n",
    "\n",
    "The distilled model is saved as part of the script as a sharded checkpoint, where each model parallel worker is resposible for saving its shard of the model weights. In order to use the model for inference, we need to consolidate the model shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli neuron consolidate ./final_chess_model ./final_chess_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results\n",
    "\n",
    "Check the training output and saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model was saved\n",
    "final_model_path = \"./final_chess_model\"\n",
    "\n",
    "if Path(final_model_path).exists():\n",
    "    print(f\"✓ Model saved to {final_model_path}\")\n",
    "    print(f\"\\nModel files:\")\n",
    "    !ls -lh {final_model_path}\n",
    "else:\n",
    "    print(f\"✗ Model not found at {final_model_path}\")\n",
    "    print(\"Training may have failed. Check the output above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully:\n",
    "- ✓ Loaded chess move evaluation dataset with teacher logits\n",
    "- ✓ Configured knowledge distillation training\n",
    "- ✓ Trained a 0.6B student model from a 30B teacher\n",
    "- ✓ Saved the trained model for inference\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to Lab 2 to test the trained model\n",
    "- Compare student vs teacher predictions\n",
    "- Measure inference speed improvements\n",
    "\n",
    "**Model Compression:**\n",
    "- Teacher: 30B parameters\n",
    "- Student: 0.6B parameters\n",
    "- **Reduction**: 50x smaller!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
