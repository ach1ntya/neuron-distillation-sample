{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "## Copy tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Specify the name of the Hugging Face model\n",
    "model_name = \"Qwen/Qwen3-0.6B\" \n",
    "\n",
    "# 2. Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 3. Define the local directory where you want to save the tokenizer\n",
    "local_directory = \"./final_chess_model\"\n",
    "\n",
    "# 4. Save the tokenizer to the local directory\n",
    "tokenizer.save_pretrained(local_directory)\n",
    "\n",
    "print(f\"Tokenizer for '{model_name}' saved to '{local_directory}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation\n",
    "\n",
    "After completing the fine-tuning process, the next step is to compile the trained model for AWS Trainium inference using the Hugging Face Optimum Neuron toolchain.\n",
    "Neuron compilation optimizes the model graph and converts it into a Neuron Executable File Format (NEFF), enabling efficient execution on NeuronCores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!optimum-cli export neuron \\\n",
    "  --model \"./final_chess_model\" \\\n",
    "  --task text-generation \\\n",
    "  --sequence_length 2048 \\\n",
    "  --batch_size 1 \\\n",
    "  ./final_chess_model_compiled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "We will install the Optimum Neuron vllm library.  Then, run inference using the compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q optimum-neuron[vllm] matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can run the batch inference example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM\n",
    "from transformers import AutoTokenizer\n",
    "from inference_helper import (\n",
    "    load_chess_data,\n",
    "    create_prompts,\n",
    "    run_inference,\n",
    "    process_inference_results,\n",
    "    save_evaluation_results,\n",
    "    print_sample_results\n",
    ")\n",
    "\n",
    "# Load the compiled model\n",
    "llm = LLM(\n",
    "    model=\"./final_chess_model_compiled\",\n",
    "    max_num_seqs=1,\n",
    "    max_model_len=2048,\n",
    "    tensor_parallel_size=2,\n",
    ")\n",
    "\n",
    "# Load chess dataset\n",
    "chess_data = load_chess_data('/home/ubuntu/environment/neuron-distillation-sample/distillation/data/chess_output.json')\n",
    "\n",
    "# Create prompts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./final_chess_model_compiled\")\n",
    "prompts = create_prompts(chess_data, tokenizer)\n",
    "\n",
    "# Run inference\n",
    "outputs = run_inference(llm, prompts, max_tokens=2048, temperature=0.0)\n",
    "\n",
    "# Process results\n",
    "results, teacher_correct, student_correct = process_inference_results(outputs, chess_data)\n",
    "\n",
    "# Save results and print summary\n",
    "summary = save_evaluation_results(results, teacher_correct, student_correct)\n",
    "\n",
    "# Show sample results\n",
    "print_sample_results(results, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from inference_helper import (\n",
    "    print_detailed_statistics,\n",
    "    create_visualization,\n",
    "    print_sample_predictions\n",
    ")\n",
    "\n",
    "# Load results\n",
    "with open('static/chess_evaluation_results.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Print detailed statistics\n",
    "print_detailed_statistics(data)\n",
    "\n",
    "# Create visualization\n",
    "create_visualization(data)\n",
    "\n",
    "# Show sample predictions\n",
    "print_sample_predictions(data, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional\n",
    "\n",
    "You can add in the results from the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start an inference endpoint on the current instance, like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model=\"/home/ubuntu/environment/ml/qwen/compiled_model\" \\\n",
    "    --max-num-seqs=1 \\\n",
    "    --max-model-len=2048 \\\n",
    "    --tensor-parallel-size=2 \\\n",
    "    --port=8080 \\\n",
    "    --device \"neuron\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And query the inference endpoint like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl 127.0.0.1:8080/v1/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -X POST \\\n",
    "    -d '{\"prompt\":\"<|im_start|>system\\n    You are a sentiment classifier. You take input strings and return the sentiment of POSITIVE, NEGATIVE, or NEUTRAL. Only return the sentiment.\\n    <|im_start|>user\\n    The service at this restaurant exceeded all my expectations!\\n    <|im_start|>assistant\", \"temperature\": 0.8, \"max_tokens\":128}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
